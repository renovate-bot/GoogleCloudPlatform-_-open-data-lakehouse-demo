{"cells": [{"cell_type": "code", "source": ["# Copyright 2025 Google LLC\n", "#\n", "# Licensed under the Apache License, Version 2.0 (the \"License\");\n", "# you may not use this file except in compliance with the License.\n", "# You may obtain a copy of the License at\n", "#\n", "#     https://www.apache.org/licenses/LICENSE-2.0\n", "#\n", "# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n", "# limitations under the License."], "metadata": {"id": "3rDg9KfcBdJP"}, "id": "3rDg9KfcBdJP", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "Hbol", "metadata": {"id": "Hbol"}, "source": ["# Ridership Open Lakehouse Demo (Part 1): Load data to BigQuery Iceberg tables\n", "\n", "This notebook will demonstrate a strategy to implement an open lakehouse on GCP, using Apache Iceberg,\n", "as an open source standard for managing data, while still leveraging GCP native capabilities. This demo will use\n", "BigQuery Manged Iceberg Tables, Managed Apache Kafka and Apache Kafka Connect to ingest streaming data, Vertex AI for Generative AI queries on top of the data and Dataplex to govern tables.\n", "\n", "This notebook will load data into BigQuery, backed by Parquet files, in the Apache Iceberg specification.\n", "\n", "All data in this notebook was prepared in the previous `part0` notebook."], "execution_count": null}, {"cell_type": "markdown", "id": "MJUe", "metadata": {"marimo": {"config": {"hide_code": true}}, "id": "MJUe"}, "source": ["## Setup the environment"], "execution_count": null}, {"cell_type": "code", "execution_count": null, "id": "vblA", "metadata": {"id": "vblA", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1753435130933, "user_tz": -120, "elapsed": 1143, "user": {"displayName": "", "userId": ""}}, "outputId": "1645a750-451d-40ba-bfaf-3f795033e3a3"}, "outputs": [], "source": ["import os\n", "USER_AGENT = \"cloud-solutions/data-to-ai-nb-v3\"\n", "\n", "PROJECT_ID = !gcloud config get-value project\n", "PROJECT_ID = PROJECT_ID[0]\n", "BQ_DATASET = \"ridership_lakehouse\"\n", "BUCKET_NAME = f\"{PROJECT_ID}-ridership-lakehouse\"\n", "LOCATION = \"us-central1\"\n", "BQ_CONNECTION_NAME = \"cloud-resources-connection\"\n", "\n", "print(PROJECT_ID)\n", "print(BUCKET_NAME)"]}, {"cell_type": "code", "execution_count": null, "id": "lEQa", "metadata": {"id": "lEQa"}, "outputs": [], "source": ["from google.cloud import bigquery, storage\n", "from google.api_core.client_info import ClientInfo\n", "\n", "bigquery_client = bigquery.Client(\n", "    project=PROJECT_ID,\n", "    location=LOCATION,\n", "    client_info=ClientInfo(user_agent=USER_AGENT)\n", ")\n", "storage_client = storage.Client(\n", "    project=PROJECT_ID,\n", "    client_info=ClientInfo(user_agent=USER_AGENT)\n", ")\n", "\n", "bucket = storage_client.bucket(BUCKET_NAME)"]}, {"cell_type": "code", "source": ["# create/reference the bq dataset, and clean all tables\n", "dataset_ref = bigquery.Dataset(f\"{PROJECT_ID}.{BQ_DATASET}\")\n", "dataset_ref.location = LOCATION\n", "dataset = bigquery_client.create_dataset(dataset_ref, exists_ok=True)\n", "\n", "for table in bigquery_client.list_tables(dataset):\n", "    bigquery_client.delete_table(table)"], "metadata": {"id": "ugEQEeAtZ1n1"}, "id": "ugEQEeAtZ1n1", "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["# Some helper functions\n", "\n", "import pandas as pd\n", "\n", "pd.set_option('display.max_colwidth', None)\n", "\n", "def display_blobs_with_prefix(prefix: str, top=20):\n", "  blobs = [[b.name, b.size, b.content_type, b.updated] for b in\n", "         storage_client.list_blobs(BUCKET_NAME, prefix=prefix, )]\n", "  df = pd.DataFrame(blobs, columns=[\"Name\", \"Size\", \"Content Type\", \"Updated\"])\n", "  return df.head(top)\n", "\n", "def delete_blobs_with_prefix(prefix: str):\n", "  blobs = storage_client.list_blobs(BUCKET_NAME, prefix=prefix)\n", "  for blob in blobs:\n", "    blob.delete()\n", "\n", "\n", "def select_top_rows(table_name: str, num_rows: int=10):\n", "  query = f\"\"\"\n", "  SELECT *\n", "  FROM `{PROJECT_ID}.{BQ_DATASET}.{table_name}`\n", "  LIMIT {num_rows}\n", "  \"\"\"\n", "  return bigquery_client.query(query).to_dataframe()"], "metadata": {"id": "DAJqwAKqaz5j"}, "id": "DAJqwAKqaz5j", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["## Create the tables and load data"], "metadata": {"id": "OWx9xx9aWpGu"}, "id": "OWx9xx9aWpGu", "execution_count": null}, {"cell_type": "markdown", "source": ["### Different types of Iceberg tables in BigQuery\n", "\n", "BigQuery offers two ways to work with Apache Iceberg tables: **BigLake External Tables for Iceberg** and **BigQuery Tables for Apache Iceberg**. For most migration and native BigQuery use cases, **BigQuery Tables for Apache Iceberg (managed by BigQuery) is the strongly preferred method.**\n", "\n", "-----\n", "\n", "**1\\. BigLake External Tables for Iceberg (Managed Externally \u2014 Read-Only)**\n", "\n", "These tables allow BigQuery to query Iceberg data managed by external systems like Spark or Hive. They are best for hybrid setups where multiple tools need read access and an external system controls the table's lifecycle.\n", "\n", "**SQL Example:**\n", "\n", "```sql\n", "CREATE OR REPLACE EXTERNAL TABLE `your-project.your_dataset.your_external_iceberg_table`\n", "  WITH CONNECTION `your-region.your_connection_name`\n", "  OPTIONS (\n", "         format = 'ICEBERG',\n", "         uris = [\"gs://mybucket/mydata/mytable/metadata/iceberg.metadata.json\"]\n", "   )\n", "```\n", "\n", "**Key Points:**\n", "\n", "  * **External Control:** Metadata and data managed outside BigQuery.\n", "  * **Read-Only:** BigQuery can only query; DML operations are not supported.\n", "  * **Hybrid Fit:** Ideal for shared access from various tools.\n", "  * **Metadata:** Manual updates for static JSON pointers; BigLake Metastore preferred for dynamic syncing in GCP.\n", "\n", "-----\n", "\n", "**2\\. BigQuery Tables for Apache Iceberg (Managed by BigQuery)**\n", "\n", "**This is the recommended approach for migrating your data and integrating Iceberg within BigQuery.** These tables offer full BigQuery management of Iceberg, eliminating the need for a separate catalog.\n", "\n", "**SQL Example:**\n", "\n", "```sql\n", "CREATE OR REPLACE TABLE `your-project.your_dataset.your_iceberg_table`(\n", "    <column_definition>\n", ")\n", "WITH CONNECTION `your-region.your_connection_name`\n", "OPTIONS (\n", "    file_format = 'PARQUET',\n", "    table_format = 'ICEBERG',\n", "    storage_uri = 'gs://your-bucket/iceberg/your_table_name'\n", ");\n", "```\n", "\n", "**Why should you prefer Managed Tables:**\n", "\n", "BigQuery-managed Iceberg tables unlock powerful features essential for modern data solutions:\n", "\n", "  * **Native Integration:** Seamless experience, similar to standard BigQuery tables.\n", "  * **Full DML Support:** Perform `INSERT`, `UPDATE`, `DELETE`, `MERGE` directly with GoogleSQL.\n", "  * **Unified Ingestion:** Supports both batch and high-throughput streaming via the Storage Write API.\n", "  * **Schema Evolution:** BigQuery handles schema changes (add, drop, rename columns, type changes) effortlessly.\n", "  * **Automatic Optimization:** Benefits from BigQuery's built-in optimizations like adaptive file sizing, clustering, and garbage collection.\n", "  * **Robust Security:** Leverage BigQuery's column-level security and data masking.\n", "  * **Simplified Operations:** Reduced overhead by letting BigQuery manage the Iceberg table lifecycle.\n", "\n", "This method provides a more robust, integrated, and efficient way to leverage Iceberg data within the BigQuery ecosystem.\n", "\n", "\n", "#### How to Choose?\n", "\n", "Generally, to leverage the most out of you Iceberg data, prefer the managed tables. They provide better integration and automatic optimization.\n", "\n", "If you have BigQuery centric pipelines, with data generated by BigQuery, managed iceberg tables are the obvious choice.\n", "\n", "Choose external tables, if you have spark centric pipelines (or another external engine) that generate and write Iceberg data in GCS, and BigQuery only requires read-only access.\n", "\n", "In a real world scenario, you will probably have some of both, so a truly unified data platform would have a mixture of both tables.\n", "\n", "In this notebook, we will create the 2 different types of tables, to demonstrate that the 2 methods can be combined according to your needs.\n", "\n", "We will generate managed tables for the `bus_stations` and `ridership` datasets, while for the `bus_lines` dataset, we will write iceberg data directly to GCS, using Apache Spark, and mount the data as an external table in BigQuery.\n"], "metadata": {"id": "z2TGzD069Kwp"}, "id": "z2TGzD069Kwp", "execution_count": null}, {"cell_type": "markdown", "source": ["### The `bus_stations` table\n", "\n", "This table will be loaded as a BigQuery Iceberg table (option 2)- managed by BigQuery, read-only access to other processing engines.\n"], "metadata": {"id": "vyt1ywHoviSE"}, "id": "vyt1ywHoviSE", "execution_count": null}, {"cell_type": "code", "source": ["bus_stops_prefix = \"iceberg_data/bus_stations\"\n", "bus_stops_uri = f\"gs://{BUCKET_NAME}/{bus_stops_prefix}/\"\n", "\n", "# Clear the GCS path before\n", "delete_blobs_with_prefix(bus_stops_prefix)\n", "display_blobs_with_prefix(bus_stops_prefix)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 53}, "id": "GDsTeoKVcA2a", "executionInfo": {"status": "ok", "timestamp": 1753444532699, "user_tz": -120, "elapsed": 879, "user": {"displayName": "", "userId": ""}}, "outputId": "03789949-6679-498e-fd44-4bc601a3a1f7"}, "id": "GDsTeoKVcA2a", "execution_count": null, "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "Xref", "metadata": {"id": "Xref", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1753444538266, "user_tz": -120, "elapsed": 1689, "user": {"displayName": "", "userId": ""}}, "outputId": "65bdc2ce-7853-4ac6-e4ec-8da63faaad9b"}, "outputs": [], "source": ["# create the table\n", "bigquery_client.query(f\"DROP TABLE IF EXISTS {BQ_DATASET}.bus_stations;\").result()\n", "query = f\"\"\"\n", "CREATE TABLE {BQ_DATASET}.bus_stations\n", "(\n", "  bus_stop_id INTEGER,\n", "  address STRING,\n", "  school_zone BOOLEAN,\n", "  seating BOOLEAN,\n", "  borough STRING,\n", "  latitude FLOAT64,\n", "  longtitude FLOAT64\n", ")\n", "WITH CONNECTION `{PROJECT_ID}.{LOCATION}.{BQ_CONNECTION_NAME}`\n", "OPTIONS (\n", "  file_format = 'PARQUET',\n", "  table_format = 'ICEBERG',\n", "  storage_uri = '{bus_stops_uri}');\n", "\"\"\"\n", "bigquery_client.query(query).result()"]}, {"cell_type": "code", "source": ["# We can view the GCS path, and see that there is now an ICEBERG metadata file, but no data\n", "display_blobs_with_prefix(bus_stops_prefix)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 143}, "id": "t_2Yf9yecodY", "executionInfo": {"status": "ok", "timestamp": 1753444706080, "user_tz": -120, "elapsed": 843, "user": {"displayName": "", "userId": ""}}, "outputId": "2ed779ea-e729-4d8b-d4b7-aa880366d07a"}, "id": "t_2Yf9yecodY", "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["# we will now load the data from the CSV in GCS\n", "\n", "# BQ tables for Apache Iceberg do not support load with truncating, so we will truncate manually, and then load\n", "truncate = bigquery_client.query(f\"DELETE FROM {BQ_DATASET}.bus_stations WHERE TRUE\")\n", "truncate.result()\n", "\n", "job_config = bigquery.LoadJobConfig(\n", "    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n", "    source_format=bigquery.SourceFormat.CSV,\n", "    skip_leading_rows=1,\n", ")\n", "\n", "job = bigquery_client.load_table_from_uri(\n", "    f\"gs://{BUCKET_NAME}/mta_staging_data/bus_stations.csv\",\n", "    dataset.table(\"bus_stations\"),\n", "    job_config=job_config,\n", ")\n", "\n", "job.result()"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Lr7eXVQicRVW", "executionInfo": {"status": "ok", "timestamp": 1753444651065, "user_tz": -120, "elapsed": 4827, "user": {"displayName": "", "userId": ""}}, "outputId": "dc31e353-28b4-4200-e938-554a8176df90"}, "id": "Lr7eXVQicRVW", "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["# We can verify that the data is actually loaded in the iceberg specification and the format used is parquet\n", "display_blobs_with_prefix(bus_stops_prefix)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 143}, "id": "qjUf5Qlbma05", "executionInfo": {"status": "ok", "timestamp": 1753444658389, "user_tz": -120, "elapsed": 379, "user": {"displayName": "", "userId": ""}}, "outputId": "a13e45cf-6e9e-4ad6-a810-8868bf1fe375"}, "id": "qjUf5Qlbma05", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["We can see in the ourput that we have 8 parquet files generated under the `iceberg_data/bus_stations/data/` folder, and one `v0.metadata.json` under the `iceberg_data/bus_stations/metadata/` folder."], "metadata": {"id": "rggvYQ7ApP2N"}, "id": "rggvYQ7ApP2N", "execution_count": null}, {"cell_type": "code", "source": ["select_top_rows(\"bus_stations\")"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 363}, "id": "FO69Buz5wf4J", "executionInfo": {"status": "ok", "timestamp": 1753450033867, "user_tz": -120, "elapsed": 2715, "user": {"displayName": "", "userId": ""}}, "outputId": "062ae4af-4a05-4c10-dc79-b4863a31f06c"}, "id": "FO69Buz5wf4J", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["### The `bus_lines` table\n", "\n", "For the `bus_lines` table, we want to simulate a table that is managed by Spark, and BigQuery is just needs to read the table.\n", "\n", "For that we will use the `EXTERNAL` Iceberg tables (method 1), managed by OSS engines, read-only by BigQuery.\n", "\n", "To simulate that, we will start a PySpark process to load the data in Iceberg format, and expose the metadata to BigQuery."], "metadata": {"id": "ofBoYgs90zqI"}, "id": "ofBoYgs90zqI", "execution_count": null}, {"cell_type": "code", "source": ["# Define environment variables or set them directly\n", "WAREHOUSE_PREFIX = \"external_iceberg_warehouse\"\n", "\n", "# make sure the destination to the warehouse is empty\n", "delete_blobs_with_prefix(WAREHOUSE_PREFIX)\n", "display_blobs_with_prefix(WAREHOUSE_PREFIX)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 53}, "id": "GFYctcePX0ph", "executionInfo": {"status": "ok", "timestamp": 1753446286737, "user_tz": -120, "elapsed": 5, "user": {"displayName": "", "userId": ""}}, "outputId": "5674c3a8-26ac-4b68-c484-75502580cf5f"}, "id": "GFYctcePX0ph", "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["from google.cloud.dataproc_spark_connect import DataprocSparkSession\n", "from google.cloud.dataproc_v1 import Session\n", "\n", "WAREHOUSE = f\"gs://{BUCKET_NAME}/{WAREHOUSE_PREFIX}/\"\n", "\n", "session = Session()\n", "\n", "catalog = \"buses\"\n", "\n", "session.runtime_config.properties[f\"spark.sql.catalog.{catalog}\"] = \"org.apache.iceberg.spark.SparkCatalog\"\n", "session.runtime_config.properties[f\"spark.sql.catalog.{catalog}.type\"] = \"hadoop\"\n", "# session.runtime_config.properties[f\"spark.sql.catalog.{catalog}.catalog-impl\"] = \"org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog\"\n", "session.runtime_config.properties[f\"spark.sql.catalog.{catalog}.gcp_project\"] = f\"{PROJECT_ID}\"\n", "session.runtime_config.properties[f\"spark.sql.catalog.{catalog}.gcp_location\"] = f\"{LOCATION}\"\n", "session.runtime_config.properties[f\"spark.sql.catalog.{catalog}.warehouse\"] = f\"{WAREHOUSE}\"\n", "\n", "\n", "# Create the Spark session. This will take some time.\n", "spark = (\n", "    DataprocSparkSession.builder\n", "      .appName(\"mount-bus-lines\")\n", "      .dataprocSessionConfig(session)\n", "      .getOrCreate()\n", ")"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "6L0bH4Gg1xa0", "executionInfo": {"status": "ok", "timestamp": 1753446299402, "user_tz": -120, "elapsed": 5, "user": {"displayName": "", "userId": ""}}, "outputId": "0b410f8d-8290-43bf-9276-b020e1ede0e1"}, "id": "6L0bH4Gg1xa0", "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["df = spark.read.format(\"parquet\").load(f\"gs://{BUCKET_NAME}/mta_staging_data/bus_lines/\")\n", "df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(f\"{catalog}.bus_lines\")\n"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 46}, "id": "aMPA2bUu2t4e", "executionInfo": {"status": "ok", "timestamp": 1753446307830, "user_tz": -120, "elapsed": 5912, "user": {"displayName": "", "userId": ""}}, "outputId": "3c9a5267-1615-49bd-d2de-bc80eb04b6fa"}, "id": "aMPA2bUu2t4e", "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["# We'll verify the blobs are written\n", "display_blobs_with_prefix(WAREHOUSE_PREFIX)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 238}, "id": "Pg5Ws2b-XmSd", "executionInfo": {"status": "ok", "timestamp": 1753446316175, "user_tz": -120, "elapsed": 9, "user": {"displayName": "", "userId": ""}}, "outputId": "c0f3df00-4f8b-429d-f429-b3c0700ab393"}, "id": "Pg5Ws2b-XmSd", "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["# Now we will mount the iceberg data as an external, read-only, table in bigquery\n", "bigquery_client.query(f\"DROP TABLE IF EXISTS {BQ_DATASET}.bus_lines;\").result()\n", "\n", "# NOTE: we are pointing directly to the v1.metadata.json file\n", "# in a real world scenario, it would be better to read the content of the \"version-hint.text\"\n", "# in order to know the latest version of the metadata\n", "bigquery_client.query(f\"\"\"\n", "CREATE OR REPLACE EXTERNAL TABLE `{BQ_DATASET}.bus_lines`\n", "  WITH CONNECTION `{PROJECT_ID}.{LOCATION}.{BQ_CONNECTION_NAME}`\n", "  OPTIONS (\n", "         format = 'ICEBERG',\n", "         uris = [\"gs://{BUCKET_NAME}/{WAREHOUSE_PREFIX}/bus_lines/metadata/v1.metadata.json\"]\n", "   )\n", "\"\"\").result()\n"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "7y1_kexmZQ9l", "executionInfo": {"status": "ok", "timestamp": 1753446820807, "user_tz": -120, "elapsed": 1731, "user": {"displayName": "", "userId": ""}}, "outputId": "d39b10dc-ffc1-4a95-d59d-5d5ac1ea748e"}, "id": "7y1_kexmZQ9l", "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["# show sample rows\n", "select_top_rows(\"bus_lines\")"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 363}, "id": "9kxivahUxGPe", "executionInfo": {"status": "ok", "timestamp": 1753450065464, "user_tz": -120, "elapsed": 2464, "user": {"displayName": "", "userId": ""}}, "outputId": "b1bb6d34-3b0c-452f-f1b6-ff10eda4056e"}, "id": "9kxivahUxGPe", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["### The `ridership` table\n", "\n", "Lastly, the `ridership` table will be loaded just like the `bus_stations` table, but this time we will [cluster](https://cloud.google.com/bigquery/docs/clustered-tables) the table by the timestamp."], "metadata": {"id": "CydSvZ5rleMq"}, "id": "CydSvZ5rleMq", "execution_count": null}, {"cell_type": "code", "execution_count": null, "id": "BYtC", "metadata": {"id": "BYtC", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1753448406007, "user_tz": -120, "elapsed": 2148, "user": {"displayName": "", "userId": ""}}, "outputId": "b9b0dbaa-3cc6-41d2-a6d3-514012e4154b"}, "outputs": [], "source": ["ridership_uri = f\"gs://{BUCKET_NAME}/iceberg_data/ridership/\"\n", "\n", "blob_list = bucket.list_blobs(match_glob=f\"{ridership_uri}/*\")\n", "blob_list = [blob for blob in blob_list]\n", "bucket.delete_blobs(blob_list)\n", "\n", "bigquery_client.query(\n", "    f'DROP TABLE IF EXISTS {BQ_DATASET}.ridership;'\n", ").result()\n", "_create_table_stmt = f\"\"\"\n", "    CREATE TABLE {BQ_DATASET}.ridership (\n", "        transit_timestamp TIMESTAMP,\n", "        station_id INTEGER,\n", "        ridership INTEGER\n", "    )\n", "    CLUSTER BY transit_timestamp\n", "    WITH CONNECTION `{PROJECT_ID}.{LOCATION}.{BQ_CONNECTION_NAME}`\n", "    OPTIONS (\n", "        file_format = 'PARQUET',\n", "        table_format = 'ICEBERG',\n", "        storage_uri = '{ridership_uri}'\n", "    );\n", "\"\"\"\n", "bigquery_client.query(_create_table_stmt).result()"]}, {"cell_type": "code", "execution_count": null, "id": "Hstk", "metadata": {"id": "Hstk", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1753448460651, "user_tz": -120, "elapsed": 25773, "user": {"displayName": "", "userId": ""}}, "outputId": "faeb5117-2892-4e88-f6c0-191a323c7f7e"}, "outputs": [], "source": ["# Load data into the table\n", "table_ref = dataset_ref.table(\"ridership\")\n", "\n", "# BQ tables for Apache Iceberg do not support load with truncating, so we will truncate manually, and then load\n", "truncate = bigquery_client.query(f\"DELETE FROM {BQ_DATASET}.ridership WHERE TRUE\")\n", "truncate.result()\n", "\n", "job_config = bigquery.LoadJobConfig(\n", "    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n", "    source_format=bigquery.SourceFormat.PARQUET,\n", ")\n", "\n", "job = bigquery_client.load_table_from_uri(\n", "    f\"gs://{BUCKET_NAME}/mta_staging_data/ridership/*.parquet\",\n", "    table_ref,\n", "    job_config=job_config,\n", ")\n", "\n", "job.result()"]}, {"cell_type": "code", "source": ["# show sample rows\n", "select_top_rows(\"ridership\")"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 363}, "id": "vzcxePPwxNi4", "executionInfo": {"status": "ok", "timestamp": 1753450087982, "user_tz": -120, "elapsed": 1743, "user": {"displayName": "", "userId": ""}}, "outputId": "c0ec9bb2-fb38-4e66-f479-c98c13355424"}, "id": "vzcxePPwxNi4", "execution_count": null, "outputs": []}], "metadata": {"colab": {"provenance": [], "name": "lakehouse_part1_load_data.ipynb", "toc_visible": true}, "language_info": {"name": "python"}, "kernelspec": {"name": "python3", "display_name": "Python 3"}}, "nbformat": 4, "nbformat_minor": 5}