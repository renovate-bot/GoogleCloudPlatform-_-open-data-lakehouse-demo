{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    " # Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ],
   "id": "7e12ae86a8afa077"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Ridership Open Lakehouse Demo (Part 3): Time-Series forecasting of ridership data\n",
    "\n",
    "This notebook will demonstrate a strategy to implement an open lakehouse on GCP, using Apache Iceberg, as an open source standard for managing data, while still leveraging GCP native capabilities. This demo will use BigQuery Manged Iceberg Tables, Managed Apache Kafka and Apache Kafka Connect to ingest streaming data, Vertex AI for Generative AI queries on top of the data and Dataplex to govern tables.\n",
    "\n",
    "This notebook will use the `bus_rides` data and ML models to generate a time-series forcasting of ridership in the future, in order to alert us when a bus about to become full.\n",
    "\n",
    "We will evaluate the models accuracy and generate future data to be used in the next chapters for real-time predictions and alerting."
   ],
   "id": "1866c235f761bd97"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup the environment",
   "id": "33b2476966205868"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "USER_AGENT = \"cloud-solutions/data-to-ai-nb-v3\"\n",
    "\n",
    "PROJECT_ID = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "BQ_DATASET = \"ridership_lakehouse\"\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-ridership-lakehouse\"\n",
    "LOCATION = \"us-central1\"\n",
    "BQ_CONNECTION_NAME = \"cloud-resources-connection\"\n",
    "\n",
    "print(PROJECT_ID)\n",
    "print(BUCKET_NAME)"
   ],
   "id": "3dc893272b3ec293"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.cloud import bigquery, storage\n",
    "from google.api_core.client_info import ClientInfo\n",
    "\n",
    "bigquery_client = bigquery.Client(\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    client_info=ClientInfo(user_agent=USER_AGENT)\n",
    ")\n",
    "storage_client = storage.Client(\n",
    "    project=PROJECT_ID,\n",
    "    client_info=ClientInfo(user_agent=USER_AGENT)\n",
    ")"
   ],
   "id": "22b77a4c8a538d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Some helper functions\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "def display_blobs_with_prefix(prefix: str, top=20):\n",
    "    blobs = [[b.name, b.size, b.content_type, b.updated] for b in\n",
    "             storage_client.list_blobs(BUCKET_NAME, prefix=prefix, )]\n",
    "    df = pd.DataFrame(blobs, columns=[\"Name\", \"Size\", \"Content Type\", \"Updated\"])\n",
    "    return df.head(top)\n",
    "\n",
    "\n",
    "def delete_blobs_with_prefix(prefix: str):\n",
    "    blobs = storage_client.list_blobs(BUCKET_NAME, prefix=prefix)\n",
    "    for blob in blobs:\n",
    "        blob.delete()\n",
    "\n",
    "\n",
    "def select_top_rows(table_name: str, num_rows: int = 10):\n",
    "    query = f\"\"\"\n",
    "  SELECT *\n",
    "  FROM `{PROJECT_ID}.{BQ_DATASET}.{table_name}`\n",
    "  LIMIT {num_rows}\n",
    "  \"\"\"\n",
    "    return bigquery_client.query(query).to_dataframe()\n",
    "\n"
   ],
   "id": "19500266476fb15a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering\n",
    "\n",
    "In our demo, we want to predict rise and spikes to the demand of bus lines. In order to do that, we will calculate a `demand_metric`, which will be high when all passengers cannot fit into a bus, and low when we have remaining capacity in a bus, so we can deploy more buses to reduce the number of passengers that cannot board.\n",
    "\n",
    "Let's assume that the metric `demand_metric` depends on a number of factors:\n",
    "  * Station\n",
    "  * Borough\n",
    "  * Bus line\n",
    "  * The Date and Time (time series)\n",
    "  \n",
    "  We will use the data generated in notebooks 1 & 2 to forecast ridership, based on these factors.\n",
    "\n",
    "  Obviously, some of these variable are related (the station and Borough are related). In a real-world scenario, this might be a bad practice, and might lead to overfitting towards a specific variable, but for the sake of the demo we will try to use these features."
   ],
   "id": "8b86085ad2acb2e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create bus rides table with features",
   "id": "24acd4fcfab85e90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "FEATURES_TABLE_NAME = \"bus_rides_features\"\n",
    "prefix = f\"iceberg_data/{FEATURES_TABLE_NAME}\"\n",
    "\n",
    "ridership_features_uri = f\"gs://{BUCKET_NAME}/{prefix}/\"\n",
    "\n",
    "bigquery_client.query(f\"DROP TABLE IF EXISTS {BQ_DATASET}.{FEATURES_TABLE_NAME};\").result()\n",
    "delete_blobs_with_prefix(prefix)\n",
    "\n",
    "query = f\"\"\"\n",
    "CREATE TABLE `{BQ_DATASET}.{FEATURES_TABLE_NAME}`\n",
    "WITH CONNECTION `{PROJECT_ID}.{LOCATION}.{BQ_CONNECTION_NAME}`\n",
    "OPTIONS (\n",
    "  file_format = 'PARQUET',\n",
    "  table_format = 'ICEBERG',\n",
    "  storage_uri = '{ridership_features_uri}'\n",
    ")\n",
    "AS\n",
    "(\n",
    "  SELECT\n",
    "  r.timestamp_at_stop,\n",
    "  r.bus_ride_id,\n",
    "  r.bus_stop_id,\n",
    "  r.bus_line_id,\n",
    "  l.bus_line,\n",
    "  r.bus_size,\n",
    "  r.total_capacity,\n",
    "  s.borough,\n",
    "  r.last_stop,\n",
    "  r.passengers_in_stop,\n",
    "  r.passengers_boarding,\n",
    "  r.passengers_alighting,\n",
    "  r.remaining_capacity,\n",
    "  r.remaining_at_stop,\n",
    "  (r.remaining_at_stop - r.remaining_capacity) AS demand_metric,\n",
    "  COALESCE(SAFE_DIVIDE(r.remaining_capacity, r.total_capacity), 0) AS remaining_capacity_percentage,\n",
    "  COALESCE(SAFE_DIVIDE(r.remaining_at_stop, r.passengers_in_stop), 0) AS passengers_left_behind_percentage\n",
    "FROM `{BQ_DATASET}.bus_rides` AS r\n",
    "LEFT JOIN `{BQ_DATASET}.bus_stations` AS s ON s.bus_stop_id = r.bus_stop_id\n",
    "LEFT JOIN `{BQ_DATASET}.bus_lines` AS l ON l.bus_line_id = r.bus_line_id\n",
    ");\n",
    "\"\"\"\n",
    "bigquery_client.query(query).result()"
   ],
   "id": "1397c2fcda841d1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "select_top_rows(FEATURES_TABLE_NAME)",
   "id": "7470f5db9ff3d578"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Visualize Projected data\n",
    "\n",
    "Let's now create a visualization for each of these features, the relationship with the `ridership` column, to see the effects of different features on the target variable."
   ],
   "id": "735a098c35812f6d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Demand per bus line over time\n",
    "Let's take a look at the last 90 days of ridership data. We'll specifically look at the `remaining_at_stop` as our metric to determined if we need to alert of high demand. The higher the `remaining_at_stop`, the more demand we have, and we might want to dispatch more buses."
   ],
   "id": "6f18f627b0883ede"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "DAYS_BACK = 90\n",
    "\n",
    "demand_per_bus_line_df = bigquery_client.query(\n",
    "    f\"\"\"\n",
    "DECLARE max_ts TIMESTAMP DEFAULT (SELECT MAX(timestamp_at_stop) FROM {BQ_DATASET}.{FEATURES_TABLE_NAME});\n",
    "SELECT bus_line, timestamp_at_stop as timestamp_at_stop, AVG(demand_metric) AS demand_metric\n",
    "  FROM `{BQ_DATASET}.{FEATURES_TABLE_NAME}`\n",
    "  WHERE timestamp_at_stop > TIMESTAMP_SUB(max_ts, INTERVAL {DAYS_BACK} DAY)\n",
    "  GROUP BY bus_line, timestamp_at_stop\n",
    "  ORDER BY bus_line, timestamp_at_stop;\n",
    "\"\"\"\n",
    ").result().to_dataframe()\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# we'll sample 20 random lines, as displaying all of them is not practical\n",
    "bus_line_ids = random.sample(list(demand_per_bus_line_df.bus_line.unique()), k=20)\n",
    "\n",
    "figure = plt.figure(figsize=(20, 6))\n",
    "plt.xlabel('Timestamp at stop')\n",
    "# Group data by station ID\n",
    "for bus_line_id in bus_line_ids:\n",
    "    station_data = demand_per_bus_line_df[demand_per_bus_line_df['bus_line'] == bus_line_id].sort_values(\n",
    "        by=\"timestamp_at_stop\"\n",
    "    )\n",
    "    # Plot ridership over time for the current bus line\n",
    "    plt.bar(station_data['timestamp_at_stop'], station_data['demand_metric'], label=bus_line_id)\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Timestamp at stop')\n",
    "plt.ylabel('Remaining Passengers')\n",
    "plt.title('Remaining Passengers Over Time by Bus Line')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "49f71ecd4437fa89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "DAYS_BACK = 90\n",
    "\n",
    "demand_per_bus_stop_df = bigquery_client.query(\n",
    "    f\"\"\"\n",
    "DECLARE max_ts TIMESTAMP DEFAULT (SELECT MAX(timestamp_at_stop) FROM {BQ_DATASET}.{FEATURES_TABLE_NAME});\n",
    "SELECT bus_stop_id, timestamp_at_stop as timestamp_at_stop, AVG(demand_metric) AS demand_metric\n",
    "  FROM `{BQ_DATASET}.{FEATURES_TABLE_NAME}`\n",
    "  WHERE timestamp_at_stop > TIMESTAMP_SUB(max_ts, INTERVAL {DAYS_BACK} DAY)\n",
    "  GROUP BY bus_stop_id, timestamp_at_stop\n",
    "  ORDER BY bus_stop_id, timestamp_at_stop;\n",
    "\"\"\"\n",
    ").result().to_dataframe()\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# we'll sample 20 random lines, as displaying all of them is not practical\n",
    "bus_stop_ids = random.sample(list(demand_per_bus_stop_df.bus_stop_id.unique()), k=20)\n",
    "\n",
    "figure = plt.figure(figsize=(20, 6))\n",
    "plt.xlabel('Timestamp at stop')\n",
    "# Group data by station ID\n",
    "for bus_stop_id in bus_stop_ids:\n",
    "    station_data = demand_per_bus_stop_df[demand_per_bus_stop_df['bus_stop_id'] == bus_stop_id].sort_values(\n",
    "        by=\"timestamp_at_stop\"\n",
    "    )\n",
    "    # Plot ridership over time for the current station\n",
    "    plt.bar(station_data['timestamp_at_stop'], station_data['demand_metric'], label=bus_stop_id)\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Timestamp at stop')\n",
    "plt.ylabel('Demand')\n",
    "plt.title('Demand Over Time by Bus Stop')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "d3d2371701a73d96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Distibution of demand per Borough (boxplot)",
   "id": "e398ac91a75716a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "average_demand_per_borough = bigquery_client.query(\n",
    "    f\"\"\"\n",
    "SELECT\n",
    "  borough,\n",
    "  APPROX_QUANTILES(demand_metric, 100)[OFFSET(0)] AS whislo,\n",
    "  APPROX_QUANTILES(demand_metric, 100)[OFFSET(25)] AS q1,\n",
    "  APPROX_QUANTILES(demand_metric, 100)[OFFSET(50)] AS med,\n",
    "  AVG(demand_metric) AS mean,\n",
    "  APPROX_QUANTILES(demand_metric, 100)[OFFSET(75)] AS q3,\n",
    "  APPROX_QUANTILES(demand_metric, 100)[OFFSET(100)] AS whishi\n",
    " FROM `{BQ_DATASET}`.`{FEATURES_TABLE_NAME}`\n",
    " GROUP BY borough\n",
    "\"\"\"\n",
    ").result().to_dataframe()\n",
    "\n",
    "lst_of_dicts = average_demand_per_borough.to_dict(orient='records')\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bxp(\n",
    "    lst_of_dicts, showfliers=False, showmeans=True,\n",
    "    label=[x[\"borough\"] for x in lst_of_dicts], meanline=True\n",
    ")\n",
    "ax.set_xticklabels([x[\"borough\"] for x in lst_of_dicts])\n",
    "\n",
    "plt.ylabel('Demand')\n",
    "plt.title('Demand Per Borough')\n",
    "\n",
    "plt.show()"
   ],
   "id": "2d84ca3ad19b1925"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Average demand per month",
   "id": "7ede3314f5a537f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate average demand per calendar month\n",
    "average_demand_per_month = bigquery_client.query(\n",
    "    f\"\"\"\n",
    "SELECT\n",
    "  EXTRACT(MONTH FROM timestamp_at_stop) AS transit_month,\n",
    "  AVG(demand_metric) AS demand_metric\n",
    "FROM\n",
    "  `{BQ_DATASET}`.`{FEATURES_TABLE_NAME}`\n",
    "GROUP BY\n",
    "  transit_month;\n",
    "\"\"\"\n",
    ").result().to_dataframe()\n",
    "\n",
    "# Sort by month to ensure correct chronological order in the plot\n",
    "average_demand_per_month = average_demand_per_month.sort_values(by='transit_month').reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))  # Adjust figure size\n",
    "\n",
    "plt.bar(average_demand_per_month['transit_month'], average_demand_per_month['demand_metric'])\n",
    "\n",
    "# Add plot labels and title\n",
    "plt.xlabel(\"Transit Month\")\n",
    "plt.ylabel(\"Average Demand\")\n",
    "plt.title(\"Average Demand per Month\")\n",
    "plt.xticks(average_demand_per_month['transit_month'])  # Ensure all months are shown on x-axis\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "1802058760bf6a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Average Demand per Day-of-Week",
   "id": "e10c14911c4514d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Group average demand by day of week\n",
    "average_demand_per_day_of_week = bigquery_client.query(\n",
    "    f\"\"\"\n",
    "SELECT\n",
    "  EXTRACT(DAYOFWEEK FROM timestamp_at_stop) AS transit_day_of_week,\n",
    "  AVG(demand_metric) AS demand_metric\n",
    "FROM\n",
    "  `{BQ_DATASET}`.`{FEATURES_TABLE_NAME}`\n",
    "GROUP BY\n",
    "  transit_day_of_week;\n",
    "\"\"\"\n",
    ").result().to_dataframe()\n",
    "\n",
    "# Sort by month to ensure correct chronological order in the plot\n",
    "average_demand_per_day_of_week = average_demand_per_day_of_week.sort_values(by='transit_day_of_week').reset_index()\n",
    "\n",
    "days_of_week = {\n",
    "    1: \"Sunday\",\n",
    "    2: \"Monday\",\n",
    "    3: \"Tuesday\",\n",
    "    4: \"Wednesday\",\n",
    "    5: \"Thursday\",\n",
    "    6: \"Friday\",\n",
    "    7: \"Saturday\"\n",
    "}\n",
    "average_demand_per_day_of_week['transit_day_of_week'] = average_demand_per_day_of_week['transit_day_of_week'].map(\n",
    "    days_of_week\n",
    ")\n",
    "plt.figure(figsize=(10, 6))  # Adjust figure size\n",
    "\n",
    "plt.bar(average_demand_per_day_of_week['transit_day_of_week'], average_demand_per_day_of_week['demand_metric'])\n",
    "\n",
    "# Add plot labels and title\n",
    "plt.xlabel(\"Transit Day of Week\")\n",
    "plt.ylabel(\"Average Demand\")\n",
    "plt.title(\"Average Demand per Day of Week\")\n",
    "plt.xticks(average_demand_per_day_of_week['transit_day_of_week'])  # Ensure all months are shown on x-axis\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "e3ffd185708bb780"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Forecast bus ridership\n",
    "\n",
    "For time-series forecasting, there are 3 main options to choose from.\n",
    "\n",
    "### ARIMA_PLUS\n",
    "This model is an enhanced version of the traditional ARIMA, offering improved performance and often including automatic parameter selection for better ease of use in forecasting univariate time series. It's a **Univariate** model, meaning, it is built to predict a target value based on only the timestamp variable. You cannot incorportate extra variables that might influence the prediction. For that reason, we will **NOT** use it here in our demo.\n",
    "\n",
    "### ARIMA_PLUS_XREG\n",
    "Extending `ARIMA_PLUS`, this model incorporates the ability to utilize external regressors (exogenous variables) to enhance the forecasting accuracy by accounting for the influence of other relevant time series. Like the `ARIMA_P{LUS` It still relies on linearity assumptions between regressors and target. It is a Multivariate model, in terms of inputs, but still forecasts a single output.)\n",
    "\n",
    "### TimesFM\n",
    "`TimesFM` is a deep learning-based forecasting model that leverages transformer architectures to capture complex temporal dependencies and long-range patterns in time series data, often excelling in multi-variate forecasting tasks. It's considered excellent at capturing complex non-linear patterns and long-range dependencies; naturally handles multivariate inputs and outputs; can perform well with large datasets.It can be, however, prone to overfitting with small datasets.\n",
    "\n",
    "## What's next?\n",
    "\n",
    "In the rest of this notebook, we will create predication using both `ARIMA_PLUS_XREG` and `TimesFM` and evaluate each, so we can comopare the performance of each of the models.\n",
    "\n",
    "We will create a summarized bus_rides table, aggregate the `demand_metric` by the hour, and use linear time gaps filling strategy to train the ARIMA_PLUS_XREG model, and evaluate both models.\n"
   ],
   "id": "84160403a5c80de2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# some constants for later use\n",
    "\n",
    "ARIMA_PLUS_XREG_MODEL_NAME = \"demand_arima\"\n",
    "ARIMA_PLUS_XREG_FORECAST_TABLE_NAME = \"arima_demand_results\"\n",
    "\n",
    "MIN_TS = \"2022-01-01T00:00:00\"\n",
    "MAX_TS = \"2024-11-30T23:59:59\"\n",
    "\n",
    "EVAL_MIN_TS = \"2024-12-01T00:00:00\"\n",
    "EVAL_MAX_TS = \"2024-12-31T23:59:59\"\n",
    "\n",
    "DAYS_FORWARD_TO_FORECAST = 7\n",
    "\n",
    "SUMMARIZED_FEATURES_TABLE = \"summarized_features\"\n",
    "INTERVALS_MINUTES = 5\n",
    "\n",
    "BUS_LINE_FOR_SAMPLING = \"P-936\"\n",
    "DAY_FOR_SAMPLING = \"2024-12-05\"\n",
    "\n",
    "TIMESFM_TABLE_NAME = \"timesfm_demand_results\"\n",
    "\n",
    "ACTUAL_VS_FORECAST_TABLE_NAME = \"actual_vs_forecast\"\n",
    "\n",
    "HORIZON = int(DAYS_FORWARD_TO_FORECAST * 24 * 60 / INTERVALS_MINUTES)\n"
   ],
   "id": "da16c573667169e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{BQ_DATASET}`.`{SUMMARIZED_FEATURES_TABLE}` AS\n",
    "\n",
    "WITH agg_rides AS (\n",
    "  SELECT\n",
    "    TIMESTAMP_BUCKET(timestamp_at_stop, INTERVAL {INTERVALS_MINUTES} MINUTE) AS time,\n",
    "    bus_line,\n",
    "    bus_stop_id,\n",
    "    borough,\n",
    "    AVG(demand_metric) AS demand\n",
    "   FROM `{BQ_DATASET}.{FEATURES_TABLE_NAME}`\n",
    "   GROUP BY time, bus_line, bus_stop_id, borough)\n",
    "\n",
    "SELECT *\n",
    "FROM GAP_FILL(\n",
    "  TABLE agg_rides,\n",
    "  ts_column => 'time',\n",
    "  bucket_width => INTERVAL {INTERVALS_MINUTES} MINUTE,\n",
    "  partitioning_columns => ['bus_line', 'bus_stop_id', 'borough'],\n",
    "  value_columns => [\n",
    "    ('demand', 'linear')\n",
    "  ]\n",
    ")\n",
    "ORDER BY time, bus_line, bus_stop_id;\n",
    "\"\"\"\n",
    "# print(query)\n",
    "bigquery_client.query(query).result()\n"
   ],
   "id": "a8ec791afa82f9e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "select_top_rows(SUMMARIZED_FEATURES_TABLE)",
   "id": "71ea4f58163dc0fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Multivariate forecasting using the ARIMA_PLUS_XREG model",
   "id": "a5484015a350041a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Train the model\n",
    "\n",
    "\n",
    "The same CREATE MODEL statement is used to train this model Many options, e.g,  `time_series_data_col`, `time_series_timestamp_col`,  `time_series_id_col` have the same meaning as for the ARIMA_PLUS model.\n",
    "\n",
    "The main difference - the ARIMA_PLUS_XREG model uses all columns besides those identified by the options above as the feature columns and uses linear regression to calculate covariate weights.\n",
    "\n",
    "For details on the additional options, explanation of the training process, and best practices when training and using the model please refer to BigQuery documentation on [the CREATE MODEL statement for ARIMA_PLUS_XREG models](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-multivariate-time-series).\n",
    "\n",
    "The next query, will create a model based on data between June 1st, 2022 and 30st of November of 2024. The limitation of date ranges is due to our data size, which can't all fit to the training limitations.\n",
    "\n",
    "We also chose a training dataset in the past, but leave some data \"unseen\" to the model, to help us evaluate its accuracy.\n",
    "\n",
    "Note, we are treating each bus line, as a separate time series dataset, using the `TIME_SERIES_ID_COL` parameter.\n"
   ],
   "id": "21f4708afd8e66cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query = f\"\"\"\n",
    "CREATE OR REPLACE MODEL `{BQ_DATASET}.{ARIMA_PLUS_XREG_MODEL_NAME}`\n",
    "OPTIONS(\n",
    "  MODEL_TYPE = 'ARIMA_PLUS_XREG',\n",
    "  TIME_SERIES_ID_COL = ['bus_line', 'bus_stop_id'],\n",
    "  TIME_SERIES_DATA_COL = 'demand',\n",
    "  TIME_SERIES_TIMESTAMP_COL = 'time',\n",
    "  AUTO_ARIMA=TRUE,\n",
    "  HORIZON={HORIZON},\n",
    "  DATA_FREQUENCY='AUTO_FREQUENCY',\n",
    "  HOLIDAY_REGION = \"US\"  -- the original dataset is from NY\n",
    ")\n",
    "AS SELECT\n",
    "    time,\n",
    "    bus_line,\n",
    "    bus_stop_id,\n",
    "    borough,\n",
    "    demand\n",
    "  FROM `{BQ_DATASET}.{SUMMARIZED_FEATURES_TABLE}`\n",
    "  WHERE\n",
    "    time BETWEEN TIMESTAMP(\"{MIN_TS}\") AND TIMESTAMP(\"{MAX_TS}\");\n",
    "\"\"\"\n",
    "# print(query)\n",
    "bigquery_client.query(query).result()\n"
   ],
   "id": "50e0936eb422986"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluating the ARIMA_PLUS_XREG model",
   "id": "f7a82da0c0206ab0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "bigquery_client.query(\n",
    "    f\"SELECT * FROM ML.ARIMA_EVALUATE(MODEL `{BQ_DATASET}.{ARIMA_PLUS_XREG_MODEL_NAME}`);\"\n",
    ").result().to_dataframe()"
   ],
   "id": "fb58efb85ea2712"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM ML.EVALUATE(MODEL `{BQ_DATASET}.{ARIMA_PLUS_XREG_MODEL_NAME}`,\n",
    "  (SELECT * FROM `{BQ_DATASET}.{SUMMARIZED_FEATURES_TABLE}`\n",
    "    WHERE time BETWEEN TIMESTAMP(\"{EVAL_MIN_TS}\") AND TIMESTAMP(\"{EVAL_MAX_TS}\")\n",
    "  )\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "bigquery_client.query(query).result().to_dataframe()"
   ],
   "id": "e2904c4907354795"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Let's save the forecast results to a table, so we can compare the 2 models later\n",
    "query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{BQ_DATASET}.{ARIMA_PLUS_XREG_FORECAST_TABLE_NAME}`\n",
    "AS\n",
    "SELECT * FROM ML.FORECAST (\n",
    "    MODEL `{BQ_DATASET}.{ARIMA_PLUS_XREG_MODEL_NAME}`,\n",
    "    STRUCT (\n",
    "      {HORIZON} AS horizon,\n",
    "      0.9 AS confidence_level\n",
    "    ),\n",
    "    (\n",
    "      SELECT\n",
    "        timestamp_at_stop AS time,\n",
    "        bus_line,\n",
    "        bus_stop_id,\n",
    "        borough\n",
    "      FROM `{BQ_DATASET}.bus_rides_features`\n",
    "    )\n",
    "  )\n",
    "\"\"\"\n",
    "#print(query)\n",
    "bigquery_client.query(query).result()\n",
    "select_top_rows(ARIMA_PLUS_XREG_FORECAST_TABLE_NAME)"
   ],
   "id": "c75e14b84e381ec3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We can even try to manually compare the forecast results with the actual requests\n",
    "# let's pick one bus line at random, and compare the forecast results during a given day to the actual results from the same day\n",
    "\n",
    "query = f\"\"\"\n",
    "WITH forecast AS (\n",
    "  SELECT * FROM ML.FORECAST (\n",
    "    MODEL `{BQ_DATASET}.{ARIMA_PLUS_XREG_MODEL_NAME}`,\n",
    "    STRUCT (\n",
    "      {HORIZON} AS horizon,\n",
    "      0.9 AS confidence_level\n",
    "    ),\n",
    "    (\n",
    "      SELECT\n",
    "        timestamp_at_stop AS time,\n",
    "        bus_line,\n",
    "        bus_stop_id,\n",
    "        borough\n",
    "      FROM `{BQ_DATASET}.bus_rides_features`\n",
    "    )\n",
    "  )\n",
    "), actual AS (\n",
    "  SELECT\n",
    "    timestamp_at_stop AS time,\n",
    "    bus_line,\n",
    "    bus_stop_id,\n",
    "    borough,\n",
    "    demand_metric as observed_value,\n",
    "    TIMESTAMP_BUCKET(timestamp_at_stop, INTERVAL {INTERVALS_MINUTES} MINUTE) AS time_bucket\n",
    "  FROM {BQ_DATASET}.bus_rides_features\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  forecast.bus_line,\n",
    "  forecast.bus_stop_id,\n",
    "  forecast.forecast_timestamp,\n",
    "  actual.time_bucket,\n",
    "  actual.time AS actual_time,\n",
    "  forecast.forecast_value,\n",
    "  actual.observed_value\n",
    "FROM forecast\n",
    "INNER JOIN actual ON\n",
    "  forecast.forecast_timestamp = time_bucket AND\n",
    "  actual.bus_line = forecast.bus_line AND\n",
    "  actual.bus_stop_id = forecast.bus_stop_id\n",
    "\n",
    "  WHERE\n",
    "  actual.bus_line = '{BUS_LINE_FOR_SAMPLING}' AND\n",
    "  actual.time_bucket BETWEEN TIMESTAMP('{DAY_FOR_SAMPLING}T00:00:00') AND\n",
    "    TIMESTAMP('{DAY_FOR_SAMPLING}T23:59:59')\n",
    "\"\"\"\n",
    "\n",
    "# print(query)\n",
    "bigquery_client.query(query).result().to_dataframe()\n"
   ],
   "id": "a56a7f624ea73ede"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## TimesFM Model\n",
    "\n",
    "BigQuery's `TimesFM` is a pretrained, zero-shot foundation model for time-series forecasting. It's a decoder-only transformer model, similar to those used for language, but adapted for time series data. TimesFM was trained on a massive dataset of billions of real-world time points, which allows it to make accurate predictions on new datasets without needing any specific training on that data. This makes it highly versatile and easy for data analysts to use directly in BigQuery with a simple SQL function like `AI.FORECAST`.\n",
    "\n",
    "In this section, we will create a table with our forecast, view the results and compare the to the results to the forecasting results from the `ARIMA_PLUS_XREG` model. Although, we don't have a built in function to evaluate the timesfm model.\n",
    "\n",
    "For the `ARIMA_PLUS_XREG` has a built-in `ML.EVALUTE` function, we will simulate the same for our `TimesFM` model, and compare the results."
   ],
   "id": "29a59c9116ce4824"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create a results table",
   "id": "9c37cdfe9669beef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a table with the timesfm forecast\n",
    "query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{BQ_DATASET}.{TIMESFM_TABLE_NAME}`\n",
    "AS\n",
    "SELECT *\n",
    "FROM\n",
    "  AI.FORECAST(\n",
    "    (\n",
    "      SELECT\n",
    "        time,\n",
    "        bus_line,\n",
    "        bus_stop_id,\n",
    "        borough,\n",
    "        demand\n",
    "      FROM `{BQ_DATASET}.summarized_features`\n",
    "      WHERE\n",
    "        time BETWEEN TIMESTAMP('{MIN_TS}') AND TIMESTAMP('{MAX_TS}')\n",
    "    ),\n",
    "    horizon => {HORIZON},\n",
    "    confidence_level => 0.95,\n",
    "    id_cols => ['bus_line', 'bus_stop_id'],\n",
    "    timestamp_col => 'time',\n",
    "    data_col => 'demand');\n",
    "\"\"\"\n",
    "\n",
    "bigquery_client.query(query).result()\n",
    "select_top_rows(TIMESFM_TABLE_NAME)"
   ],
   "id": "2c10a98accde5e73"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### View results against observed results",
   "id": "65f059bfb00c0515"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Now we can review the forecast against the actual observed data\n",
    "\n",
    "query = f\"\"\"\n",
    "WITH forecast AS (\n",
    "  SELECT\n",
    "    bus_line,\n",
    "    bus_stop_id,\n",
    "    forecast_timestamp,\n",
    "    forecast_value,\n",
    "    prediction_interval_lower_bound,\n",
    "    prediction_interval_upper_bound\n",
    "  FROM `{BQ_DATASET}.{TIMESFM_TABLE_NAME}`\n",
    "), actual AS (\n",
    "  SELECT\n",
    "    timestamp_at_stop AS time,\n",
    "    bus_line,\n",
    "    bus_stop_id,\n",
    "    borough,\n",
    "    demand_metric as observed_value,\n",
    "    TIMESTAMP_BUCKET(timestamp_at_stop, INTERVAL {INTERVALS_MINUTES} MINUTE) AS time_bucket\n",
    "  FROM {BQ_DATASET}.bus_rides_features\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  forecast.bus_line,\n",
    "  forecast.bus_stop_id,\n",
    "  forecast.forecast_timestamp,\n",
    "  actual.time_bucket,\n",
    "  actual.time AS actual_time,\n",
    "  forecast.forecast_value,\n",
    "  actual.observed_value\n",
    "FROM forecast\n",
    "LEFT JOIN actual ON\n",
    "  forecast.forecast_timestamp = actual.time AND\n",
    "  actual.bus_line = forecast.bus_line AND\n",
    "  actual.bus_stop_id = forecast.bus_stop_id\n",
    "  WHERE actual.bus_line = '{BUS_LINE_FOR_SAMPLING}'\n",
    "ORDER BY forecast_timestamp\n",
    "\"\"\"\n",
    "# print(query)\n",
    "bigquery_client.query(query).result().to_dataframe()"
   ],
   "id": "da7bba2b546e8d1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Comparing results from both models",
   "id": "f9e0e11099a968f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Finally, let's compare results of the forecast\n",
    "# between the arima model and our timesfm model\n",
    "\n",
    "# We'll start off by joining the 2 forecast tables with the actual data\n",
    "# and save the results as a new table.\n",
    "query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{BQ_DATASET}.{ACTUAL_VS_FORECAST_TABLE_NAME}` AS\n",
    "\n",
    "WITH timesfm AS (\n",
    "  SELECT\n",
    "    bus_line,\n",
    "    bus_stop_id,\n",
    "    forecast_timestamp,\n",
    "    forecast_value as timesfm_forecast_value,\n",
    "  FROM `{BQ_DATASET}.{TIMESFM_TABLE_NAME}`\n",
    "), arima AS (\n",
    "  SELECT\n",
    "    bus_line,\n",
    "    bus_stop_id,\n",
    "    forecast_timestamp,\n",
    "    forecast_value as arima_forecast\n",
    "    FROM `{BQ_DATASET}.{ARIMA_PLUS_XREG_FORECAST_TABLE_NAME}`\n",
    "), actual AS (\n",
    "  SELECT\n",
    "    timestamp_at_stop AS time,\n",
    "    bus_line,\n",
    "    bus_stop_id,\n",
    "    borough,\n",
    "    demand_metric as observed_value,\n",
    "    TIMESTAMP_BUCKET(timestamp_at_stop, INTERVAL {INTERVALS_MINUTES} MINUTE) AS time_bucket\n",
    "  FROM {BQ_DATASET}.bus_rides_features\n",
    ")\n",
    "SELECT\n",
    "  actual.time AS actual_time,\n",
    "  actual.bus_line,\n",
    "  actual.bus_stop_id,\n",
    "  actual.borough,\n",
    "  actual.observed_value,\n",
    "  actual.time_bucket,\n",
    "  timesfm.timesfm_forecast_value,\n",
    "  arima.arima_forecast,\n",
    "  ABS(actual.observed_value - timesfm.timesfm_forecast_value) AS timesfm_abs_error,\n",
    "  ABS(actual.observed_value - arima.arima_forecast) AS arima_abs_error\n",
    "FROM actual\n",
    "LEFT JOIN timesfm ON\n",
    "  actual.time_bucket = timesfm.forecast_timestamp AND\n",
    "  actual.bus_line = timesfm.bus_line AND\n",
    "  actual.bus_stop_id = timesfm.bus_stop_id\n",
    "LEFT JOIN arima ON\n",
    "  actual.time_bucket = arima.forecast_timestamp AND\n",
    "  actual.bus_line = arima.bus_line AND\n",
    "  actual.bus_stop_id = arima.bus_stop_id\n",
    "WHERE\n",
    "  arima.arima_forecast IS NOT NULL AND\n",
    "  timesfm.timesfm_forecast_value IS NOT NULL\n",
    "ORDER BY actual.time, actual.bus_line, actual.bus_stop_id\n",
    "\"\"\"\n",
    "# print(query)\n",
    "bigquery_client.query(query).result().to_dataframe()\n",
    "actual_vs_forecast_df = select_top_rows(ACTUAL_VS_FORECAST_TABLE_NAME)\n",
    "actual_vs_forecast_df.head()"
   ],
   "id": "52943f2d1b3d6d0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# prompt: generate a line plot with x-axis for the `time_bucket` value, and 3 lines with different colors - 1. the actual_value 2. the `timesfm_forecast_value` 3. the `arima_forecast_value`.\n",
    "# aggregate all values to avg\n",
    "\n",
    "# Aggregate data for plotting\n",
    "plot_df = actual_vs_forecast_df.groupby('time_bucket').agg(\n",
    "    {\n",
    "        'observed_value': 'mean',\n",
    "        'timesfm_forecast_value': 'mean',\n",
    "        'arima_forecast': 'mean'\n",
    "    }\n",
    ").reset_index()\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(plot_df['time_bucket'], plot_df['observed_value'], label='Observed Value', marker='o')\n",
    "plt.plot(plot_df['time_bucket'], plot_df['timesfm_forecast_value'], label='TimesFM Forecast', marker='x')\n",
    "plt.plot(plot_df['time_bucket'], plot_df['arima_forecast'], label='ARIMA Forecast', marker='s')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Time Bucket')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Observed vs. Forecasted Values Over Time')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "da1d77587224bacd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Conclusion",
   "id": "6fde1430a2f64114"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this notebook, we used our open data lakehouse and connected it to BigQueryML and VertexAI to leverage native Google Cloud's capabilities to enhace our workflow, without losing mobility and freedom of our data.\n",
    "\n",
    "We've created tried 2 time-series forecasting models, and compared the results. We've seen the TimesFM model being the clearly better model for our demo. That shuold not take away from trying out more options. Even here, the ARIMA_PLUS_XREG model can still be tweaked and changed in order to improve it's accuracy.\n",
    "\n",
    "In the next sections, we will use the TimesFM model to perform near-real-time predictions to anticipate spikes given new data from buses."
   ],
   "id": "314a33f00554edbb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "colab": {
   "provenance": [],
   "name": "lakehouse_part3_time_series_forecasting.ipynb",
   "toc_visible": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
